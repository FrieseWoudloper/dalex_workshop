---
title: "DALEX"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: "Learn to explore, explain and examine classification models with DALEX using the Titanic dataset."
---

```{r setup, include=FALSE, message=FALSE}
library(learnr)
library(archivist)
library(DALEX)
library(DALEXtra)
library(rms)
library(randomForest)
library(gbm)
library(e1071)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, error = FALSE)

# Data
titanic  <- archivist::aread("pbiecek/models/27e5c")
johnny_d <- archivist::aread("pbiecek/models/e3596")
henry    <- archivist::aread("pbiecek/models/a6538")
 
# Train models
titanic_lrm <- archivist::aread("pbiecek/models/58b24")
titanic_rf  <- archivist::aread("pbiecek/models/4e0fc")
titanic_gbm <- archivist::aread("pbiecek/models/b7078")
titanic_svm <- archivist::aread("pbiecek/models/9c27f")

# Create explainer objects
titanic_lrm_exp <- explain(titanic_lrm,
                           data = titanic[, -9],
                           y = titanic$survived == "yes",
                           label = "Logistic Regression",
                           type = "classification",
                           verbose = FALSE)
titanic_rf_exp <- explain(model = titanic_rf, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Random Forest",
                       verbose = FALSE)
titanic_gbm_exp <- explain(model = titanic_gbm, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Generalized Boosted Regression",
                       verbose = FALSE)
titanic_svm_exp <- explain(model = titanic_svm, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Support Vector Machine",
                       verbose = FALSE)

# Explanations
lime_rf <- predict_surrogate(explainer = titanic_rf_exp, 
                             new_observation = johnny_d,
                             size = 1000,
                             seed = 1,
                             type = "localModel")
```

## 1 Welcome

This tutorial will teach you how to explore, explain and examine predictive models using the [DALEX](https://modeloriented.github.io/DALEX/) package in R. DALEX is short for mo<b>D</b>el <b>A</b>gnostic <b>L</b>anguage for <b>E</b>xploration and e<b>X</b>planation.

### 1.1 Learning objectives

We will cover:    

+ How to obtain insight into the overall behavior of a model.
+ How to obtain insight into a model's prediction for a single observation.
+ How to examine model performance and understanding the weaknesses.

We will only use _model-agnostic_ methods. These methods treat machine learning models as black boxes. They do not need access to model internals. They work by changing the input of the model and measuring changes in the prediction output.

This tutorial focuses solely on explainability of classification models. However, DALEX can just as easily be used to explain regression models.

This tutorial is based on the book [Explanatory Model Analysis](https://pbiecek.github.io/ema/) by Przemyslaw Biecek and Tomasz Burzykowski. It is a great resource and highly recommended. You can read it for free online. The book will also be available in print from early 2021.

### 1.2 Pre-requisites 
We assume you have basic knowledge of R, specifically of training machine learning models.

Let's get started!

## 2 Titanic dataset
We will use the `titanic` dataset. It contains the details of a subset of persons aboard the Titanic when it sank: 

* *gender*, the person's gender;
* *age*, the person's age in years; 
* *class*, the class in which the passenger travelled, or the duty class of a crew member;
* *embarked*, the harbor in which the person embarked on the ship;
* *country*, personâ€™s home country;
* *fare*, the price of the ticket;
* *sibsp*, the number of siblings/spouses aboard the ship;
* *parch*, the number of parents/children aboard the ship;
* *survived*, the person's survival of the disaster, _yes_ or _no_.

Click **Run Code** to see the first six rows of the dataset.   

```{r ex1, exercise = TRUE}
head(titanic)
```

In the next steps of this tutorial we will create two different kind of models. They both predict the probability of survival aboard the Titanic. So `survived` is the dependent variable. 

## 3 Models

### 3.1 Logistic regression model

It makes sense to start with a logistic regression model. We use restricted cubic splines to model the non-linear effect of age on the odds of survival.     

Click **Run Code** to train the model.

```{r ex2, exercise = TRUE}
library(rms)
titanic_lrm <- lrm(survived == "yes" ~ class + gender + rcs(age) + sibsp + parch + fare + embarked, 
                   data = titanic)
cat("The logistic regression model has been created.")
```

The logistic model is interpretable-by-design. We also create a number of so called 'black-box' models.

### 3.2 Random forest model

Our first black-box model is a random forest model. Insert the name of the dependent variable in the code chunk. Run the code. Click the **Solution** button to get help.

```{r ex3, exercise = TRUE}
library(randomForest)
set.seed(1313)
titanic_rf <- randomForest(___ ~ class + gender + age + sibsp + parch + fare + embarked, 
                           data = titanic)
cat("The random forest model has been created.")
```

```{r ex3-solution}
library(randomForest)
set.seed(1313)
titanic_rf <- randomForest(survived ~ class + gender + age + sibsp + parch + fare + embarked, 
                           data = titanic)
cat("The random forest model has been created.")
```

### 3.3 Gradient boosting model

Additionally, we create a gradient boosting model. This may take some time.

```{r ex4, exercise = TRUE, exercise.timelimit = 120}
library(gbm)
set.seed(1313)
titanic_gbm <- gbm(survived == "yes" ~ class + gender + age + sibsp + parch + fare + embarked, 
                   data = titanic, 
                   n.trees = 15000, 
                   distribution = "bernoulli")
cat("The gradient boosting model has been created.")
```

### 3.4 Support vector machine model

Finally, we also create a support vector machine (SVM) model. 

```{r ex5, exercise = TRUE}
library(e1071)
set.seed(1313)
titanic_svm <- svm(survived == "yes" ~ class + gender + age + 
                 sibsp + parch + fare + embarked, data = titanic, 
                 type = "C-classification", probability = TRUE)
cat("The support vector machine model was created.")
```


## 4 Models' predictions

Let's predict the probability of survival for two passengers aboard the Titanic: Johnny D and Henry. 

### 4.1 Johnny D

For convenience, a dataset with Johnny D's data has already been created.

```{r ex6, exercise = TRUE, exercise.eval = TRUE}
print(johnny_d)
```

As we can see, Johnny D is an 8-year-old boy who embarked in Southampton and travelled in the first class with no parents nor siblings, and with a ticket costing 72 pounds.

We obtain the predicted probability of survival for Johnny D for the different models with the generic function `predict()`.

```{r ex7, exercise = TRUE}
pred_lrm <- predict(titanic_lrm, johnny_d, type = "fitted")
pred_rf  <- predict(titanic_rf,  johnny_d, type = "prob")
pred_gbm <- predict(titanic_gbm, johnny_d, type = "response", n.trees = 15000)
pred_svm <- predict(titanic_svm, johnny_d, probability = TRUE)
cat("Predicted probability of survival for Johnny D\n",
    "==============================================\n",
    sprintf("Logistic regression model: %.*f\n", 2, pred_lrm),
    sprintf("Random forest model: %.*f\n", 2, pred_rf[1, "yes"]),    
    sprintf("Gradient boosting model: %.*f\n", 2, pred_gbm), 
    sprintf("Support vector machine model: %.*f\n", 2, attr(pred_svm, "probabilities")[1,"TRUE"]), 
    sep = "")
```

The predictions differ substantially. What causes these differences? Which model deserves our trust? Let's try and find the answers to these questions!

### 4.2 Henry

Henry is an 47-year-old man who embarked in Cherbourg and travelled in the first class with no parents nor siblings, and with a ticket costing 25 pounds.

```{r ex8, exercise = TRUE, exercise.eval = TRUE}
print(henry)
```

For Henry we also predict the odds of survival using different models.

```{r ex9, exercise = TRUE}
pred_lrm <- predict(titanic_lrm, henry, type = "fitted")
pred_rf  <- predict(titanic_rf,  henry, type = "prob")
pred_gbm <- predict(titanic_gbm, henry, type = "response", n.trees = 15000)
pred_svm <- predict(titanic_svm, henry, probability = TRUE)
cat("Predicted probability of survival for Henry\n",
    "==============================================\n",
    sprintf("Logistic regression model: %.*f\n", 2, pred_lrm),
    sprintf("Random forest model: %.*f\n", 2, pred_rf[1, "yes"]),    
    sprintf("Gradient boosting model: %.*f\n", 2, pred_gbm), 
    sprintf("Support vector machine model: %.*f\n", 2, attr(pred_svm, "probabilities")[1,"TRUE"]), 
    sep = "")
```

```{r q1}
question("How does the prediction for Henry compare to that for Johnny D?",
  answer("Henry's predicted probability of survival is <i>lower</i>.", correct = TRUE),
  answer("The predicted probabilities of survival for Henry and Johnny D are <i>equal</i>."),
  answer("Henry's predicted probability of survival is <i>higher</i>.")
)
```

## 5 Models' explainers

The models trained in the previous step have different internal structures and interfaces. We need a unified interface, so that we can easily obtain explanations and compare the models. For this purpose we create _model explainer objects_. 

We use the function `explain()` from the `DALEX` package to create an explainer object. The function arguments that we use in this tutorial are:

* `model` (required), the model to be explained;
* `data`, data to which the model is to be applied, be sure to remove the dependent variable; 
* `y`, observed values of the dependent variable corresponding to the data given in the `data` object;
* `label`, unique name of the model;
* `type`, information about the type of the model, either `"classification"` or `"regression"`,
* `verbose`, logical indicating whether diagnostic messages are to be printed.

Let's start with an explainer for the logistic regression model. Complete the code block below and run it.

```{r ex10, exercise = TRUE}
library(DALEX)
titanic_lrm_exp <- explain(model = ___,
                           data = titanic[, -9],
                           y = ___$survived == "yes",
                           label = "Logistic Regression",
                           type = ___)
```

```{r ex10-solution}
library(DALEX)
titanic_lrm_exp <- explain(model = titanic_lrm,
                           data = titanic[, -9],
                           y = titanic$survived == "yes",
                           label = "Logistic Regression",
                           type = "classification")
```

An explainer object is a list with all elements needed to generate model explanations. It contains the model, features, ground truth labels, predictions and more. 

In the code chunk below, use the code completion functionality to select the list element containing the name and version of the package that was used to train the model. 

```{r ex11, exercise = TRUE}
titanic_lrm_exp$___
```

```{r ex11-solution}
titanic_lrm_exp$model_info
```

```{r q2}
l <- unlist(strsplit(titanic_lrm_exp$model_info$ver, "[.]"))
create_answer <- function(){
  index <- sample(1:3, 1)
  l[index] <<- as.character(as.integer(l[index]) + 1)
  return(paste(l, collapse = "."))
}
question("What version of the <code>rms</code> package was used to create the logistic regression model?",
         answer(create_answer()),
         answer(titanic_lrm_exp$model_info$ver, correct = TRUE),  
         answer(create_answer()),
         answer(create_answer())
)
```

We also create explainer objects for the other models.

``````{r ex12, exercise = TRUE}
titanic_rf_exp <- explain(model = titanic_rf, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Random Forest",
                       verbose = FALSE)
titanic_gbm_exp <- explain(model = titanic_gbm, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Generalized Boosted Regression",
                       verbose = FALSE)
titanic_svm_exp <- explain(model = titanic_svm, 
                       data = titanic[, -9],
                       y = titanic$survived == "yes", 
                       label = "Support Vector Machine",
                       verbose = FALSE)
cat("The explainers have been created.")
```

## 6 Instance level explanations

An _instance level explanation_ helps us understand the model response (prediction) at a single point (instance). Sometimes it's also called a _local explanation_. There are different approaches to instance level explanations, which we will consider in turn.

One approach is to analyze how the modelâ€™s prediction for a particular instance differs from the average prediction, and to which explanatory variables this difference can be attributed. It is often called the _variable attributions_ approach. Break-down plots and SHAP are examples of the variable attributions approach.

Another approach is to use a simpler glass-box model to approximate the black-box model around the point (instance) of interest. The glass-box model is interpretable-by-design. It describes the local behaviour of the model. LIME is an example of this approach.

Yet another approach is to investigate how changing the value of a single explanatory variable affects the response of the model for a particular instance. Ceteris-paribus profiles are very helpful in performing these _What-if analyses_.

### 6.1 Break-down plots

Break-down plots decompose a modelâ€™s prediction into contributions that are attributed to different explanatory variables.

We create a break-down plot for Johnny D's prediction obtained from the random forest model. For this we use the function `predict_parts()`. The function requires three arguments:

* `explainer`, an explainer object;
* `new_observation`, an observation to be explained;
* `type`, the method for calculation of variable attribution, for example `"break_down"`, `"break_down_interactions"` or `"shap"`.

```{r ex14, exercise = TRUE, exercise.eval = TRUE}
bd_rf <- predict_parts(explainer = titanic_rf_exp, 
                       new_observation = johnny_d, 
                       type = "break_down")
plot(bd_rf)
```

The first row in the break-down plot shows the overall mean value of predictions for the entire dataset. 

The next rows present the changes in the mean prediction when fixing values of subsequent explanatory variables. They are the contributions attributed to the explanatory variables, and can be positive (green) or negative (red). 

The last row contains the prediction for the particular instance of interest. It is the sum of the overall mean value and the changes.

The random forest model predicts a higher probability of survival for Johnny D than Henry. How can we explain this difference? Create a second break-down plot for Henry and compare it to Johnny D's in order to answer this question.

```{r ex15, exercise = TRUE}

```
```{r ex15-solution}
plot(predict_parts(titanic_rf_exp, johnny_d, type = "break_down"), 
     title = "Break-down plot for Johnny D", min_max = c(0.2, 0.8))
plot(predict_parts(titanic_rf_exp, henry, type = "break_down"), 
     title = "Break-down plot for Henry", min_max = c(0.2, 0.8))
```

```{r q5}
question("According to these break-down plots, what is the main reason for the difference in predicted probability of survival?",
  answer("Johnny D's ticket was more expensive than Henry's."),
  answer("Johnny D embarked in Southampton and Henry in Cherbourg."),
  answer("Johnny D is a child and Henry middle-aged.", TRUE),
  answer("Johnny D travels with his parents and Henry alone.")
)
```

These break-down plots only show the <u>additive</u> attributions. They may be misleading for models including interactions, which is the case for our random forest model. In the presence of interactions, the size and sign of contributions depend upon the ordering of the explanatory variables that are used in calculations. To address this issue, we can create [break-down plots for interactions](http://ema.drwhy.ai/iBreakDown.html) or use methods like SHAP or LIME.

### 6.2 SHAP

<b>SH</b>apley <b>A</b>dditive ex<b>P</b>lanations (SHAP) are based on _Shapley values_ in cooperative game theory. They are a way to address the ordering issue mentioned in the previous section. The idea is to average the value of a variable's contribution over all possible orderings. 

Complement the code below and create an explanation for Johnny D's prediction using Shapley values. It takes too much time to calculate the Shapely values for _all_ orderings. So we compute them for 25 random orderings.

```{r ex16, exercise = TRUE, exercise.timelimit = 600}
set.seed(1313)
shap_rf <- predict_parts(explainer = ___, 
                       new_observation = ___, 
                       type = "shap",
                       B = 25)
plot(shap_rf, show_boxplots = FALSE)
```

```{r ex16-solution}
set.seed(1313)
shap_rf <- predict_parts(explainer = titanic_rf_exp, 
                         new_observation = johnny_d, 
                         type = "shap",
                         B = 25)
plot(shap_rf, show_boxplots = FALSE)
```

```{r q6}
question("What are practical limitations of SHAP? Select all answers that apply.",
  answer("Exact computation of Shapley values takes a lot of computing time.", TRUE),
  answer("There is no SHAP implementation in Python."),
  answer("It is necessary to have access to the training data or another representative dataset.", TRUE)
)
```

### 6.3 LIME

Break-down plots and Shapley values are not well-suited for models with a very large number of explanatory variables. Because all variables are used to explain the prediction, it gets too complicated. If we need _sparse_ explanations <b>L</b>ocal <b>I</b>nterpretable <b>M</b>odel-agnostic <b>E</b>xplanations (LIME) are a better approach.

The first step is to create a lower-dimensional data space. Continuous data is segmented and sorted into bins. Categorical data is aggregated.

The next step is to generate artificial data. This is done by using perturbations of the instance of interest. 

Finally a linear model is fitted to the artificial data. It is a glass-box model that locally approximates the black-box model. To further limit the number of variables in the glass-box model, regularization techniques like LASSO are used.

Since the Titanic dataset is already low-dimensional, we probably wouldn't use LIME. The method is usually only applied to models for high-dimensional data, like text and images. 

However, as an example, we use the LIME method to explain Johnny D's prediction from the random forest model. For that we need to call the `predict_surrogate` function from the `DALEXtra` package. We use the following function arguments:

* `explainer`, an explainer object;
* `new_observation`, an observation to be explained;
* `size`, the number of artificial data points to be generated;
* `seed`, the seed for the random number generator;
* `type`, the implementation of the LIME method to be used, for example `"localModal"`. 

```{r ex17, exercise = TRUE}
library(DALEXtra)
lime_rf <- predict_surrogate(explainer = titanic_rf_exp, 
                             new_observation = johnny_d,
                             size = 1000,
                             seed = 1,
                             type = "localModel")
plot(lime_rf)
```

```{r q7}
question("Why is the glass-box model easier to explain than the black-box model? Select all answers that apply.",
  answer("It is a linear model, which is interpretable-by-design.", TRUE),
  answer("It has fewer explanatory variables and thus is a sparser model.", TRUE),
  answer("It has been trained on a lower-dimensional dataset.", TRUE),
  answer("It more accurately fits the training data.")
)
```

### 6.4 Ceteris-paribus profiles

Ceteris-paribus profiles help us understand how changes in the values of the explanatory variable affect the modelâ€™s predictions, assuming that the values of all other variables remain the same.

For creating ceteris-paribus profiles, we need the function `predict_profile()`. In this tutorial we use the following arguments:

* `explainer`, an explainer object;
* `new_observation`, an observation to be explained,
* `variables`, names of the explanatory variables for which centeris-paribus profiles are to be calculated. 

Let's create some ceteris-paribus profiles for Johnny D's prediction from the random forest model.
```{r ex20, exercise = TRUE}
cp_titanic_rf <- predict_profile(explainer = titanic_rf_exp, 
                                 new_observation = johnny_d,
                                 variables = c("age", "class"))
plot(cp_titanic_rf, variables = "age")  
plot(cp_titanic_rf, variables = "class", variable_type = "categorical", categorical_type = "bars")
```

```{r q8}
question("Pretend Johnny D can change his age. At what age are his odds of survival lowest, all else being equal?",
  answer("1"),
  answer("3"),
  answer("41", TRUE),
  answer("65")
)
```


```{r q9}
question("It is better for Johnny not to sail as a passenger, but as a crew member. Where should he land a job?",
  answer("In the restaurant."),
  answer("In the engine room."),
  answer("In the launderette."),
  answer("On deck.", TRUE)
)         
```

```{r ex21, exercise = TRUE}
library(ggplot2)
cp_titanic_lrm <- predict_profile(titanic_lrm_exp, henry)
cp_titanic_rf  <- predict_profile(titanic_rf_exp,  henry)
plot(cp_titanic_lrm, cp_titanic_rf, color = "_label_",  
     variables = "age") + ggtitle("Ceteris-paribus profiles for Henry", "")
```

## 7 Dataset level explanations
