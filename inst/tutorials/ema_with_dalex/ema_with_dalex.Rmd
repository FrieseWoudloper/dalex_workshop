---
title: "DALEX"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: "Learn to explore and explain supervised machine learning models with DALEX."
---

```{r setup, include=FALSE, message=FALSE, results='hide'}
library(learnr)
library(gradethis)
library(DALEX)
library(ranger)
library(rms)

knitr::opts_chunk$set(echo = FALSE, exercise.checker = gradethis::grade_learnr)

set.seed(1313)
titanic_rf <- ranger(survived ~ ., data = titanic_imputed,
                          classification = TRUE, probability = TRUE)
exp_rf <- explain(titanic_rf,
                      data = titanic_imputed[, 1:7],
                      y = titanic_imputed$survived,
                      label = "Random Forest")
mp_rf <- model_performance(exp_rf)

titanic_lms <- lrm(survived ~ rcs(age)*gender + rcs(fare) + class,
                   data = titanic_imputed)
exp_lms <- explain(titanic_lms,
                   data = titanic_imputed,
                   y = titanic_imputed$survived,
                   label = "Logistic with splines")
mp_lms <- model_performance(exp_lms)

joe <- titanic_imputed[1,]
johnny_d <- data.frame(
          class = factor("1st", levels = c("1st", "2nd", "3rd", 
                     "deck crew", "engineering crew", 
                     "restaurant staff", "victualling crew")),
          gender = factor("male", levels = c("female", "male")),
          age = 8, sibsp = 0, parch = 0, fare = 72,
          embarked = factor("Southampton", levels = c("Belfast",
                      "Cherbourg","Queenstown","Southampton")))

lucy <- data.frame(
      class = factor("1st", levels = c("1st", "2nd", "3rd", 
                     "deck crew", "engineering crew", 
                     "restaurant staff", "victualling crew")),
      gender = factor("female", levels = c("female", "male")),
      age = 18, sibsp = 0, parch = 0, fare = 70,
      embarked = factor("Southampton", levels = c("Belfast",
                        "Cherbourg","Queenstown","Southampton")))

bd_joe <- predict_parts(exp_rf, joe, type = "break_down_interactions")
bd_lucy <- predict_parts(exp_rf, lucy, type = "break_down_interactions")
sh_joe_rf <- predict_parts(exp_rf, joe, type = "shap")
sh_joe_lms <- predict_parts(exp_lms, joe, type = "shap")
```

## Welcome

This tutorial will teach you how to explore and explain model behavior using the [DALEX](https://modeloriented.github.io/DALEX/) package.

### Learning objectives

The tutorial focuses on explainability of classification models. 

We will cover:    

+ How to create an _explainer_ for a model
+ How to create an explanation for the overall behavior of a model (_global explanation_)
+ How to create an explanation for individual predictions (_local explanation_)
+ How to compare explanations for different kind of models
+ How to visualize the trade-off between accuracy and explainability

We will only be using _model-agnostic_ methods. These methods treat machine learning models as black boxes. They do not need access to model internals. They work by changing the input of the model and measuring changes in the prediction output.

### Pre-requisites 
We assume you have basic knowledge of training and evaluating machine learning models in R.

Let's get started!

## Titanic dataset
For the next exercises we will use the `titanic_imputed` dataset. It is a list of people who were on board the Titanic when it sank. Run `?DALEX::titanic_imputed` in R for detail information about the dataset.

To see the first six rows of the dataset click on **Run Code**.   

```{r data-titanic, exercise=TRUE, exercise.eval=FALSE, exercise.lines=3}
head(titanic_imputed)
```

The variable `survived` specifies whether the person has survived the sinking: Yes (1) or No (0).

In the next steps we will create and explain a model that predicts whether a person will survive.

## Model & explainer

### Create a random forest model
Insert the name of the target variable in the code chunk and run the code to train a random forest model. You can click on the **Solution** button to get help.

```{r rf-titanic, exercise=TRUE, exercise.eval=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(ranger)
set.seed(1313)
titanic_rf <- ranger(___ ~ ., data = titanic_imputed,
                     classification = TRUE, probability = TRUE)
titanic_rf
```

```{r rf-titanic-solution}
library(ranger)
titanic_rf <- ranger(survived ~ ., data = titanic_imputed,
                     classification = TRUE, probability = TRUE)
titanic_rf
```

### Create an explainer

Create a model explainer using the `explain` function from the `DALEX` package.

The first argument to the function is the model you want to explain. It is the model you created in the previous step. The second argument is the data frame with the features, and the third a numeric vector with the true outcome of the target variable. The last one is the name of the model. It can be anything you like.

```{r explain-rf-titanic, exercise=TRUE, exercise.eval=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(DALEX)
exp_rf <- explain(___,
                  data = titanic_imputed[, 1:7],
                  y = ___$survived,
                  label = "Random Forest")
```

```{r explain-rf-titanic-solution}
library(DALEX)
exp_rf <- explain(titanic_rf,
                  data = titanic_imputed[, 1:7],
                  y = titanic_imputed$survived,
                  label = "Random Forest")
```

After the explainer is created, you can use it to request base information about the model.

```{r rf-model-info-titanic, exercise=TRUE, exercise.eval=FALSE, exercise.lines=3}
exp_rf$model_info
```

HIER NOG WAT TUSSEN OVER DE STRUCTUUR VAN HET EXPLAINER OBJECT?

Change this code chunk so it no longer returns the true outcomes, but the predicted probabilities. 

```{r rf-preds-titanic, exercise=TRUE, exercise.eval=FALSE}
head(exp_rf$y)
```

```{r rf-preds-titanic-solution}
head(exp_rf$y_hat)
```

### Create a logistic regression model and accompanying explainer

Let's also train a logistic regression model with splines, so we can choose the best model. Again create an explainer, so we can later on compare the two models not only on accuracy, but also explainability.

```{r rms-titanic, exercise=TRUE, exercise.eval=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(rms)
titanic_lms <- lrm(survived ~ rcs(age)*gender + rcs(fare) + class,
                   data = titanic_imputed)
exp_lms <- explain(___,
                   data = ___,
                   y = ___,
                   label = "Logistic with splines")
```

```{r rms-titanic-solution}
library(rms)
set.seed(123)
titanic_lms <- lrm(survived ~ rcs(age)*gender + rcs(fare) + class,
                   data = titanic_imputed)
exp_lms <- explain(titanic_lms,
                   data = titanic_imputed,
                   y = titanic_imputed$survived,
                   label = "Logistic with splines")
```

Extract the model base information from the explainer.

```{r rms-model-info-titanic, exercise=TRUE, exercise.eval=FALSE, warning=FALSE, message=FALSE, error=FALSE}
exp_lms$___
```

```{r rms-model-info-titanic-solution}
exp_lms$model_info
```

## Model performance

Let's see how both models perform on the training set using the `model_performance` function. Of course, you should evaluate performance on a testing dataset. However, to keep this tutorial simple, we'll skip that part for now.

```{r mp-rf-titanic, exercise=TRUE, exercise.eval=TRUE, exercise.lines=3}
model_performance(exp_rf)
```

<div id="mp-rf-titanic-hint">
**Hint:** You may want to compare the output of `model_performance(exp_rf)` to `model_performance(exp_lms)`.
</div>

```{r mp-titanic, echo=FALSE}
question("Which model we trained in the previous section seems to perform better?",
  answer("Random forest model", correct = TRUE),
  answer("Logistic regression model with splines"),
  answer("Both models perform equally")
)
```

We can visualize and compare model performance using the `plot` function. By providing an `geom` argument we can specify how we want to summarize residuals. Valid values are `"prc"`, `"roc"`, `"ecdf"`, `"boxplot"`, `"gain"`, `"lift"` and `"histogram"`.

Adjust the code below so that an _Receiver Operator Characteristic_ (ROC) curve is created, and we can verify that the random forest model has a larger _Area Under the Curve_ (AUC) and thus performs better.

```{r roc-titanic, exercise=TRUE, exercise.eval=TRUE}
mp_rf <- model_performance(exp_rf)
mp_lms <- model_performance(exp_lms)
plot(mp_rf, mp_lms, geom = "prc")
```

```{r roc-titanic-solution}
mp_rf <- model_performance(exp_rf)
mp_lms <- model_performance(exp_lms)
plot(mp_rf, mp_lms, geom = "roc")
```

## Explanations for overall model behavior

## Explanations for predictions

### Make predictions

Let us now compare predictions that are obtained from the two different models. In particular, we compute the predicted probability of survival for Joe.

Joe is a 42-year-old male who embarked in Southampton and travels third class with no parents nor siblings, and with a ticket costing a little over 7 pounds.

```{r henry-titanic, exercise=TRUE, exercise.eval=FALSE}
joe <- titanic_imputed[1,]
joe
```

First predict the probability of Joe surviving using the random forest model.

```{r pred-joe-titanic, exercise=TRUE, exercise.eval=TRUE}
predict(exp_rf, joe)
```

<div id="pred-joe-titanic-hint">
**Hint:** You probably want to compare the output of `predict(exp_rf, joe)` to `predict(exp_lms, joe)`.
</div>

We can also predict survival using the logistic regression model.

```{r joe-titanic, echo=FALSE}
question("Which model predicts a slightly higher probability of survival for Joe?",
  answer("Random forest model", correct = TRUE),
  answer("Logistic regression model with splines")
)
```

### Create break-down plots 

Both our models predict that it is not very likely that Joe will survive the sinking of the Titanic. Which characteristics of Joe were of most importance here? One way of answering this question is by creating a _break-down plot for additive attributions_. The plot is a decomposition of the model's prediction into contributions that can be attributed to different explanatory variables.

```{r bd-joe-titanic, exercise=TRUE, exercise.eval=TRUE}
bd_joe <- predict_parts(exp_rf, joe, type = "break_down_interactions")
plot(bd_joe)
```

From the break-down profile we can see, that the fact that Joe is a man decreases his chances of survival significantly.

Another passenger aboard the Titanic is Lucy. She is a 18-year-old girl who also embarked in Southampton and travels first class without parents nor siblings. Her ticket costs 70 pounds. A dataframe containing Lucy's characteristics has already been created for our convenience.

```{r pred-lucy-titanic, exercise=TRUE, exercise.eval=TRUE}
predict(exp_rf, lucy)
```

```{r pred-lucy-titanic-solution}
bd_lucy <- predict_parts(exp_rf, lucy, type = "break_down_interactions")
plot(bd_lucy)
```

Our random forest model predicts that Lucy will very likely survive. Now let's answer the following question using this model and a break-down plot. Add some code to the chunk above to get the right answer.

```{r lucy-titanic, echo=FALSE}
question("Which three characteristics of Lucy have the biggest <u>positive</u> contribution to her predicted survival?",
  answer("age = 18"),
  answer("class = 1st", correct = TRUE),  
  answer("embarked = Southampton"),
  answer("fare = 70", correct = TRUE),
  answer("gender = female", correct = TRUE),
  answer("number of siblings aboard = 0"),
  answer("number of parents aboard = 0")
)
```

### Important issue with break-down plots

Break-down plots are easy to understand, and they are compact. However, they can also be misleading. This is the case for models including interactions. Break-down plots only show the additive attributions. Thus the choice of the ordering of the explanatory variables in the calculation of the variable-importance measures is important. Let's illustrate this with an example.

```{r bd-order-titanic, exercise=TRUE, exercise.eval=TRUE}
bd_lucy_order <- predict_parts(exp_rf, lucy, 
                               order = c("age", "gender", "fare", "class",
                                         "parch", "sibsp", "embarked"))
plot(bd_lucy_order)
```

We can see that fare&nbsp;=&nbsp;70 now has a <u>negative</u> contribution to the prediction, and that the positive contribution of class&nbsp;=&nbsp;1st has significantly increased!

There are several ways of addressing the issue of the dependence of the variable-importance measure on the ordering of the explanatory variables. One approach is using SHAP, as we will see in the next section.

### Create SHapley Additive exPlanations

Another way to explain individual predictions is by creating _SHapley Additive exPlanations_ (SHAP). 

Let's start by visualizing SHAP values for the random forest model's prediction for Joe.

```{r sh-joe-rf-titanic, exercise=TRUE, exercise.eval=TRUE}
sh_joe_rf <- predict_parts(exp_rf, joe, type = "shap")
plot(sh_joe_rf, show_boxplots = FALSE)
```

Now also visualize Joe's SHAP values based upon the logistic regression model, so we can compare not only the predictions, but also the explanations of the two different models.

```{r sh-joe-lms-titanic, exercise=TRUE, exercise.eval=FALSE, warning=FALSE, message=FALSE, error=FALSE}
sh_joe_lms <- predict_parts(___, joe, type = "shap")
plot(___, show_boxplots = FALSE)
```

```{r sh-joe-lms-titanic-solution}
sh_joe_lms <- predict_parts(exp_lms, joe, type = "shap")
plot(sh_joe_lms, show_boxplots = FALSE)
```

What are the most striking differences between the explanations for Joe's prediction for the two different models?

### Ceteris-paribus

